---
title: "Untitled"
author: "RRiddell"
date: "06/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(corrr)
library(caret)
library(vip)
```

```{r eval=FALSE, include=FALSE}
source("funs/data_cleaner.R")
```

```{r}
# read in data
all_data <- read_csv("out/all_data.csv")
game_total <- read_csv("out/game_total.csv")
```

```{r}
# splitting data into training and testing groups
inTrain <- createDataPartition(y = game_total$results, p = 0.7, list = F)
training <-  game_total %>% 
  slice(inTrain)

testing <-  game_total %>% 
  slice(-inTrain)

testing_countries <- game_total %>% 
  slice(-inTrain) %>% 
  select(country)
```

```{r}
# analysing the distribution of the points variable
ggplot(training, aes(x = pts, y =..density..)) +
  geom_histogram(colour = 'violet', fill = 'violet', alpha = 0.8, binwidth = 10) +
  geom_density(fill = 'black', alpha = 0.3) +
  geom_vline(xintercept = mean(training$score_margin), linetype = "dashed", colour = "blue")
```


```{r}
# estbalishing what categorical variables to assess via a box plot
cat_vars <- c("year", "tournament_stage", "competition", "results")

# function and for loop to view the boxplot
gg_box <- function(x){
  ggplot(game_total, aes(factor(.data[[x]]), pts)) +
  geom_boxplot()
}

for (i in cat_vars){
  print(gg_box(i))
}

```


```{r}
# extracting the numeric variables for the scatter plot to view the relationship
num_vars <- training %>% 
  select(mp:mean_PER, -results) %>% 
  colnames(.)

# Function and for loop to visualise all the scatterplots
gg_point <- function(y){
  ggplot(training, aes(pts, .data[[y]])) +
  geom_point() +
    geom_text(x = max(training$pts) *.90, 
              y = max(training[[y]]) *.90, 
              aes(label = paste("R:",round(cor(training$pts, .data[[y]]),2))))
}

for (i in num_vars){
  print(gg_point(i))
}

```

```{r}
# establish a correlation matrix
cor_mat <- training %>% 
  select(where(is.numeric),-pts) %>% 
  cor(., method = 'spearman')

## select the variables that display multicolinearty above 0.8
cor_features <- findCorrelation(cor_mat, cutoff = 0.8, names = T, exact = FALSE)

## remove the variables that display multicolinearty
## leaving the variables that best represent the correlation
training <- training %>% 
  select(-cor_features)

```

```{r}
# find the correlation of all the varibles
ordered_colnames <- training %>% 
  select(where(is.numeric)) %>% 
  correlate() %>% 
  focus(pts) %>% 
  mutate(pts = abs(pts)) %>% 
  arrange(pts)

# Remove those that have a correlation of under 0.2
low_cor <- ordered_colnames %>% 
  filter(pts < .2) %>% 
  pull(term) 
training <- training %>% 
  select(!low_cor)
```


```{r}
# Remove multi colinear variables
training <- training %>% 
  select(!c(x3pa, x3P_p))

# assess the corrlation matrix visually
GGally::ggcorr(training, method = c('pairwise','spearman'), 
               size = 2 )

# test for near zero variance
nearZeroVar(training, names = T)
# Test for skewness
e1071::skewness(training$pts)
Skewness <- training %>%
  select(where(is.numeric)) %>% 
  summarise(across(c(x3p:tsa),e1071::skewness)) %>% 
  gather(Variable, Skewness) %>% 
  arrange(-Skewness)
```

```{r}
# create some dummy objetcs that can be used in modelling
# these are used to comvert categorical factors in numeric factors
dummy_obj <- dummyVars(pts ~ . , data = training)
training_withDummies <- predict(dummy_obj, newdata = training)
training_withDummies <- bind_cols(training_withDummies, pts = training$pts)
names(training_withDummies) <- str_replace_all(string = names(training_withDummies),
                                               pattern = " ", 
                                               replace= "_")
```

```{r}
# establish a cross validation object
control_obj <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 5
  )
```

```{r}
# perform the linear multiple regression model
library(MASS)
set.seed(1)
lm_mdl <- train(pts ~ . , data =  training, 
                method = 'lmStepAIC',
                trControl = control_obj,
                trace = F)

broom::tidy(lm_mdl$finalModel)
lm_mdl
detach("package:MASS")

linmod <- lm(pts ~ ., data = training)
backward <- step(linmod, direction = "backward", trace = 0)
vi(linmod)

```

```{r}
# apply a decsion tree model to the testing data
set.seed(1)
colnames(training_withDummies) <- make.names(colnames(training_withDummies))
tree_mdl <- train(pts ~ . , data = training_withDummies,
                  method = 'rpart',
                  trControl = control_obj,
                  tuneGrid = expand.grid(cp = seq(0.001,0.015,0.001)))

tree_mdl
plot(tree_mdl)
rattle::fancyRpartPlot(tree_mdl$finalModel, sub = "")
plot(varImp(tree_mdl), top = 20)

```

```{r}
# Random forest model
set.seed(1)
rf_mdl <- train(pts ~ ., data = training_withDummies,
                method = 'rf',
                trControl = control_obj)

rf_mdl
plot(rf_mdl)
plot(varImp(rf_mdl), top = 20)
```

```{r}
# K nearest neighbour model
set.seed(1)
knn_mdl <-  train(pts~ ., data = training, method = "knn",
                  preProc = c('center', 'scale'),
                  tuneGrid = data.frame(.k = 1:60),
                  trControl = control_obj)

knn_mdl

```

```{r}
# Compare the model output
resampls <- resamples(list(LM = lm_mdl,
                           TREE = tree_mdl,
                           RF = rf_mdl,
                           KNN = knn_mdl))
## displaying the model perfomance in order to select the best model
gridExtra::grid.arrange(bwplot(resampls, metric = 'RMSE'),
                        bwplot(resampls, metric = 'R Squared'),
                        ncol = 2)

```

```{r}
## using the linear model to predcit the points scored in the testing
testing <-  testing %>%
  mutate(predictions = predict(lm_mdl, newdata = testing))

## calculting the RMSE amd R squared of the linear model on the testing data
RMSE(pred = testing$predictions,
     obs = testing$pts)
R2(pred = testing$predictions,
   obs = testing$pts)


## plotting the predictions against the actual points from the testing data
testing <- testing %>% 
  mutate(resids = predictions - pts,
         resids = abs(resids),
         assign_label = if_else(resids >= 5, 1,0),
         label = paste("Country:", country,"\n",
                       "Actual Points:", round(pts), "\n",
                       "Predicted Points:", round(predictions)))
  


pts_analysis_pred_act <- ggplot(testing, aes(predictions, pts, label = label)) +
  geom_point(colour = '#ACE894', alpha=0.7) +
  geom_abline(colour = "pink", linetype = "dashed") +
  theme_classic() +
  theme(text = element_text(size= size_axis),
        plot.title = element_text(size = size_title, hjust = 0.5)) +
  labs(title = "Predicted Points vs Actual Points",
       x = "Predicted Points",
       y = "Actual Points",
       fill = "Final Classification") +
  ggrepel::geom_label_repel(aes(label = if_else(assign_label == 1, label, "")), size = 2.5)


ggsave("figs/pts_analysis_pred_act.jpeg",  pts_analysis_pred_act, device = "jpeg", width = 15)

```

